{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98a5a862-8426-42a2-a70f-0804c5a35ca6",
   "metadata": {},
   "source": [
    "This file demonstrates how the Station data is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27de6779-f36a-4011-8838-97ffea09f55c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (2.32.2)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\anaconda\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests) (2024.6.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\anaconda\\lib\\site-packages (from beautifulsoup4) (2.5)\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b68a911-78d7-498c-97f2-b371a7e9d069",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (2.32.2)\n",
      "Collecting requests\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests) (2024.6.2)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "   ---------------------------------------- 0.0/64.9 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 10.2/64.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 64.9/64.9 kB 1.2 MB/s eta 0:00:00\n",
      "Installing collected packages: requests\n",
      "Successfully installed requests-2.32.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b0646543-fec0-4667-9291-22dbb9f671c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b407e9c-a907-4441-877d-8943215c89b2",
   "metadata": {},
   "source": [
    "Step 1: Define URLs for London Tube lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e4a0fd22-a6ff-424c-bf56-4dc3c077a79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These URLs are used to scrape station information for each tube line\n",
    "base_url = \"https://tfl.gov.uk/tube/route/\"\n",
    "elizabeth_url = \"https://tfl.gov.uk/elizabeth-line/route/elizabeth\"\n",
    "lines = [\n",
    "    {\"name\": \"bakerloo\", \"url\": f\"{base_url}bakerloo/\"},\n",
    "    {\"name\": \"central\", \"url\": f\"{base_url}central/\"},\n",
    "    {\"name\": \"circle\", \"url\": f\"{base_url}circle/\"},\n",
    "    {\"name\": \"district\", \"url\": f\"{base_url}district/\"},\n",
    "    {\"name\": \"elizabeth-line\", \"url\": elizabeth_url},  # Special case for Elizabeth Line\n",
    "    {\"name\": \"hammersmith-city\", \"url\": f\"{base_url}hammersmith-city/\"},\n",
    "    {\"name\": \"jubilee\", \"url\": f\"{base_url}jubilee/\"},\n",
    "    {\"name\": \"metropolitan\", \"url\": f\"{base_url}metropolitan/\"},\n",
    "    {\"name\": \"northern\", \"url\": f\"{base_url}northern/\"},\n",
    "    {\"name\": \"piccadilly\", \"url\": f\"{base_url}piccadilly/\"},\n",
    "    {\"name\": \"victoria\", \"url\": f\"{base_url}victoria/\"},\n",
    "    {\"name\": \"waterloo-city\", \"url\": f\"{base_url}waterloo-city/\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8043fd-7e30-4030-b25b-8a5b2c4b3b92",
   "metadata": {},
   "source": [
    "Step 2: Scrape Tube station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "30a4f3ce-ca48-4b27-8c92-a5c38013d711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store station information\n",
    "all_stations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "79d51f66-22f2-4d05-916b-dc4b0ff4f022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping line: bakerloo\n",
      "Scraping line: central\n",
      "Scraping line: circle\n",
      "Scraping line: district\n",
      "Scraping line: elizabeth-line\n",
      "Scraping line: hammersmith-city\n",
      "Scraping line: jubilee\n",
      "Scraping line: metropolitan\n",
      "Scraping line: northern\n",
      "Scraping line: piccadilly\n",
      "Scraping line: victoria\n",
      "Scraping line: waterloo-city\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each line to scrape data\n",
    "for line in lines:\n",
    "    try:\n",
    "        # Send a request to the line's URL\n",
    "        response = requests.get(line[\"url\"])\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            print(f\"Scraping line: {line['name']}\")\n",
    "\n",
    "            # Extract station data from the HTML\n",
    "            stations = soup.select(\".stop-list li\")\n",
    "            for station in stations:\n",
    "                # Extract station name and StopNaptanId\n",
    "                station_name = station.select_one(\".stop-link\").get_text(strip=True) if station.select_one(\".stop-link\") else None\n",
    "                stop_naptan_id = station.select_one(\".stopNaptanId\")\n",
    "                stop_naptan_id = stop_naptan_id[\"value\"] if stop_naptan_id else None\n",
    "\n",
    "                if station_name and stop_naptan_id:\n",
    "                    # Clean the station name to remove unwanted content\n",
    "                    station_name = re.sub(r\"\\s{3}.*\", \"\", station_name).strip()\n",
    "                    station_name = re.sub(r\"Connect.*\", \"\", station_name).strip()\n",
    "                    \n",
    "                    # Append the cleaned data to the list\n",
    "                    all_stations.append({\n",
    "                        \"Line\": line[\"name\"].capitalize(),\n",
    "                        \"Station\": station_name,\n",
    "                        \"StopNaptanId\": stop_naptan_id\n",
    "                    })\n",
    "        else:\n",
    "            print(f\"Failed to fetch data for {line['name']} line, status code: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while scraping {line['name']}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b83c2cf-ca2b-4ee7-9d54-c5a632477132",
   "metadata": {},
   "source": [
    "Step 3: Save station data to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "41305df6-ec8d-467b-bc93-3445a1763f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of dictionaries into a DataFrame\n",
    "stations_df = pd.DataFrame(all_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "35677410-c0f5-4324-9069-d76e5a77a89e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Line</th>\n",
       "      <th>Station</th>\n",
       "      <th>StopNaptanId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bakerloo</td>\n",
       "      <td>Elephant &amp; Castle Underground Station</td>\n",
       "      <td>940GZZLUEAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bakerloo</td>\n",
       "      <td>Lambeth North Underground Station</td>\n",
       "      <td>940GZZLULBN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bakerloo</td>\n",
       "      <td>Waterloo Underground Station</td>\n",
       "      <td>940GZZLUWLO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bakerloo</td>\n",
       "      <td>Embankment Underground Station</td>\n",
       "      <td>940GZZLUEMB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bakerloo</td>\n",
       "      <td>Charing Cross Underground Station</td>\n",
       "      <td>940GZZLUCHX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Line                                Station StopNaptanId\n",
       "0  Bakerloo  Elephant & Castle Underground Station  940GZZLUEAC\n",
       "1  Bakerloo      Lambeth North Underground Station  940GZZLULBN\n",
       "2  Bakerloo           Waterloo Underground Station  940GZZLUWLO\n",
       "3  Bakerloo         Embankment Underground Station  940GZZLUEMB\n",
       "4  Bakerloo      Charing Cross Underground Station  940GZZLUCHX"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the DataFrame\n",
    "stations_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f0753571-41fa-43df-bedf-ab893b5d218c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a timestamped filename for the output Excel file\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "stations_file = f\"london_tube_stations_{timestamp}.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7b75e325-78d4-444a-8ced-392d0e813942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station data saved to 'london_tube_stations_20250128_144703.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the DataFrame to an Excel file\n",
    "stations_df.to_excel(stations_file, index=False)\n",
    "print(f\"Station data saved to '{stations_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6516b9d8-2986-4478-aeaa-b074a531e301",
   "metadata": {},
   "source": [
    "Step 4: Fetch latitude and longitude using TfL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "832aa099-ecb8-4dd0-87b3-ebfa6903631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store API results\n",
    "api_results = []\n",
    "tfl_api_url = \"https://api.tfl.gov.uk/StopPoint/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f68d4a17-5669-43fd-b24f-f48a44c36c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the station data from the previously saved Excel file\n",
    "stations_df = pd.read_excel(stations_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b7db7564-959e-48e8-87b5-f0e5854f7c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching data for StopNaptanId: 940GZZLUMHL: HTTPSConnectionPool(host='api.tfl.gov.uk', port=443): Max retries exceeded with url: /StopPoint/940GZZLUMHL?app_id=I&app_key=154d17a84edd4f5ba7c2350b556b3157 (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1000)')))\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each station to fetch location data\n",
    "for index, row in stations_df.iterrows():\n",
    "    stop_id = row['StopNaptanId']\n",
    "    try:\n",
    "        app_id = \"I\"\n",
    "        app_key = \"154d17a84edd4f5ba7c2350b556b3157\"\n",
    "\n",
    "        # Send a request to the TfL API\n",
    "        response = requests.get(f\"{tfl_api_url}{stop_id}?app_id={app_id}&app_key={app_key}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            lat = data.get(\"lat\", None)\n",
    "            lon = data.get(\"lon\", None)\n",
    "            api_results.append({\n",
    "                \"StopNaptanId\": stop_id,\n",
    "                \"Station\": row['Station'],\n",
    "                \"Latitude\": lat,\n",
    "                \"Longitude\": lon\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Failed to fetch data for StopNaptanId: {stop_id}, status code: {response.status_code}\")\n",
    "            api_results.append({\n",
    "                \"StopNaptanId\": stop_id,\n",
    "                \"Station\": row['Station'],\n",
    "                \"Latitude\": None,\n",
    "                \"Longitude\": None\n",
    "            })\n",
    "\n",
    "        # Add a delay to avoid hitting API rate limits\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for StopNaptanId: {stop_id}: {e}\")\n",
    "        api_results.append({\n",
    "            \"StopNaptanId\": stop_id,\n",
    "            \"Station\": row['Station'],\n",
    "            \"Latitude\": None,\n",
    "            \"Longitude\": None\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74df1c1b-2806-46f5-bb01-852cd7552a27",
   "metadata": {},
   "source": [
    "Step 5: Save API results to DataFrame and display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aaa311af-4723-4cb5-aa4e-00aed73de6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the API results into a DataFrame\n",
    "output_df = pd.DataFrame(api_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3e66146e-dfac-4c12-9647-a9468c391803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StopNaptanId</th>\n",
       "      <th>Station</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>940GZZLUEAC</td>\n",
       "      <td>Elephant &amp; Castle Underground Station</td>\n",
       "      <td>51.494505</td>\n",
       "      <td>-0.099185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>940GZZLULBN</td>\n",
       "      <td>Lambeth North Underground Station</td>\n",
       "      <td>51.498808</td>\n",
       "      <td>-0.112315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>940GZZLUWLO</td>\n",
       "      <td>Waterloo Underground Station</td>\n",
       "      <td>51.504269</td>\n",
       "      <td>-0.113356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>940GZZLUEMB</td>\n",
       "      <td>Embankment Underground Station</td>\n",
       "      <td>51.507058</td>\n",
       "      <td>-0.122666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>940GZZLUCHX</td>\n",
       "      <td>Charing Cross Underground Station</td>\n",
       "      <td>51.507819</td>\n",
       "      <td>-0.126137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  StopNaptanId                                Station   Latitude  Longitude\n",
       "0  940GZZLUEAC  Elephant & Castle Underground Station  51.494505  -0.099185\n",
       "1  940GZZLULBN      Lambeth North Underground Station  51.498808  -0.112315\n",
       "2  940GZZLUWLO           Waterloo Underground Station  51.504269  -0.113356\n",
       "3  940GZZLUEMB         Embankment Underground Station  51.507058  -0.122666\n",
       "4  940GZZLUCHX      Charing Cross Underground Station  51.507819  -0.126137"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the DataFrame\n",
    "output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ced7c8e3-5229-4ee5-b087-5e9f59c1da2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a timestamped filename for the output Excel file\n",
    "output_file = f\"Stations_with_LatLon_{timestamp}.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8677c2ce-86dc-408e-9271-960eabaca42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latitude and Longitude data saved to 'Stations_with_LatLon_20250128_144703.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Save the results to an Excel file\n",
    "output_df.to_excel(output_file, index=False)\n",
    "print(f\"Latitude and Longitude data saved to '{output_file}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
